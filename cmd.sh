RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-hwmoe    --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  --combine_backward  2>&1 | tee TeleChat-2B-A05B-hwmoe.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-hwmoe-p2 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  --combine_backward  --apply_hwmoe_training_interval 2  2>&1 | tee TeleChat-2B-A05B-hwmoe-p2.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-exp16    --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  2>&1 | tee TeleChat-2B-A05B-exp16.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp8/  --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-exp8     --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  2>&1 | tee TeleChat-2B-A05B-exp8.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp4/  --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-exp4     --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  2>&1 | tee TeleChat-2B-A05B-exp4.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp2/  --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-exp2     --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  2>&1 | tee TeleChat-2B-A05B-exp2.log


RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-hwmoe-p2-continue --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  --combine_backward  --apply_hwmoe_training_interval 2  --resume_from_checkpoint /gemini/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-hwmoe-p2/checkpoint-5000/  2>&1 | tee TeleChat-2B-A05B-hwmoe-p2-continue.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  ./saves/TeleChat-2B-A05B-exp16-continue    --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  --resume_from_checkpoint /gemini/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-exp16/checkpoint-9000/  2>&1 | tee TeleChat-2B-A05B-exp16-continue.log


RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-hwmoe-p2 --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train  --combine_backward  --apply_hwmoe_training_interval 2  2>&1 | tee TeleChat-2B-A05B-hwmoe-p2.log
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16/ --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-exp16    --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train   2>&1 | tee TeleChat-2B-A05B-exp16.log
# param1
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe-param1/  --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-hwmoe-param1  --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train   2>&1 | tee TeleChat-2B-A05B-hwmoe-param1.log
# param2
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe-param2/  --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-hwmoe-param2  --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train   2>&1 | tee TeleChat-2B-A05B-hwmoe-param2.log

# TO Train
RES_OPTIONS="no-reverse" NCCL_IB_HCA=^mlx5_2 deepspeed  --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe-independent-attn-dense/  --train_from_scratch  --trust_remote_code --dataset default --tokenized_path /gemini/space/thu/receieve_wangzihan/Megatron-LM-core_r0.6.0_dropout_HWMoE/converted_dataset   --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-hwmoe-independent-attn-dense  --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train   2>&1 | tee TeleChat-2B-A05B-hwmoe-independent-attn-dense.log



# 下载ultra fine web
# /gemini-3/space/thu/hehaowei/huggingface_cache
MODELSCOPE_CACHE
HF_DATASETS_CACHE
NCCL_IB_HCA=^mlx5_2 deepspeed src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/base_models/qwen/Qwen3-0.6B/  --train_from_scratch  --trust_remote_code --dataset ultrafineweb --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/ultrafileweb_download  --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --flash_attn auto --do_train


# 优于MMLU效果差，
# telechat 2BA05B 在ultrafineweb上的实验
HF_DATASETS_CACHE=/gemini-3/space/thu/hehaowei/huggingface_cache NCCL_IB_HCA=^mlx5_2 deepspeed --hostfile hostfile_hwmoe src/train.py  --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16-finewebedu100bt/  --train_from_scratch  --trust_remote_code --dataset finewebedu100bt --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-exp16-finewebedu100bt/  --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192  --tokenized_path /gemini-3/space/thu/hehaowei/llamafactory_tokenized_path/finewebedu100bt --flash_attn auto --do_train --preprocessing_num_workers 60  --cache_dir /gemini-3/space/thu/hehaowei/huggingface_cache --overwrite_cache    2>&1 | tee TeleChat-2B-A05B-exp16-finewebedu100bt.log
80044348 ProcessGroupNCCL.cpp:629] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=18000000) ran for 18000044 milliseconds before timing out






# max grad norm=0.5
# expert 16
HF_DATASETS_CACHE=/gemini-3/space/thu/hehaowei/huggingface_cache NCCL_IB_HCA=^mlx5_2 deepspeed --hostfile hostfile_hwmoe src/train.py  --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16-finewebedu100bt/  --train_from_scratch  --trust_remote_code --dataset finewebedu100bt --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-exp16-finewebedu100bt/  --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 300000 --bf16 --cutoff_len 8192  --tokenized_path /gemini-3/space/thu/hehaowei/llamafactory_tokenized_path/finewebedu100bt --flash_attn auto --do_train --preprocessing_num_workers 60 --max_grad_norm 0.5    2>&1 | tee TeleChat-2B-A05B-exp16-finewebedu100bt.log
# debug
HF_DATASETS_CACHE=/gemini-3/space/thu/hehaowei/huggingface_cache NCCL_IB_HCA=^mlx5_2 deepspeed src/train.py  --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16-finewebedu100bt/  --train_from_scratch  --trust_remote_code --dataset finewebedu100bt --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/tmp  --per_device_train_batch_size 1 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --ddp_timeout 300000 --bf16 --cutoff_len 8192  --tokenized_path /gemini-3/space/thu/hehaowei/llamafactory_tokenized_path/finewebedu100bt --flash_attn auto --do_train --preprocessing_num_workers 60 --max_grad_norm 0.5

# qwen 0.6B
transformers 版本不对，现在是transformers                      4.50.0需要更新。
HF_DATASETS_CACHE=/gemini-3/space/thu/hehaowei/huggingface_cache NCCL_IB_HCA=^mlx5_2 deepspeed --hostfile hostfile_hwmoe src/train.py  --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/base_models/qwen/Qwen3-0.6B  --train_from_scratch  --trust_remote_code --dataset finewebedu100bt --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/Qwen3-0.6B-finewebedu100bt/  --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 300000 --bf16 --cutoff_len 8192  --tokenized_path /gemini-3/space/thu/hehaowei/llamafactory_tokenized_path/finewebedu100bt_qwen3_06B --flash_attn auto --do_train --preprocessing_num_workers 60 --max_grad_norm 0.5    2>&1 | tee Qwen3-06B-finewebedu100bt.log
# 更改transformer版本，创建新环境llamafactory_qwen3
DISABLE_VERSION_CHECK=1 HF_DATASETS_CACHE=/gemini-3/space/thu/hehaowei/huggingface_cache NCCL_IB_HCA=^mlx5_2 deepspeed --hostfile hostfile_hwmoe src/train.py  --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/base_models/qwen/Qwen3-0.6B  --train_from_scratch  --trust_remote_code --dataset finewebedu100bt --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/Qwen3-0.6B-finewebedu100bt/  --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 300000 --bf16 --cutoff_len 8192  --tokenized_path /gemini-3/space/thu/hehaowei/llamafactory_tokenized_path/finewebedu100bt_qwen3_06B --flash_attn auto --do_train --preprocessing_num_workers 60 --max_grad_norm 0.5    2>&1 | tee Qwen3-06B-finewebedu100bt.log
# qwen3 moe


# hwmoe
HF_DATASETS_CACHE=/gemini-3/space/thu/hehaowei/huggingface_cache NCCL_IB_HCA=^mlx5_2 deepspeed --hostfile hostfile_hwmoe src/train.py  --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage telechat_hwmoe_pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-hwmoe-finewebedu100bt/  --train_from_scratch  --trust_remote_code --dataset finewebedu100bt --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-hwmoe-finewebedu100bt/  --per_device_train_batch_size 2 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 300000 --bf16 --cutoff_len 8192  --tokenized_path /gemini-3/space/thu/hehaowei/llamafactory_tokenized_path/finewebedu100bt --flash_attn auto --do_train --preprocessing_num_workers 60 --max_grad_norm 0.5    2>&1 | tee TeleChat-2B-A05B-hwmoe-finewebedu100bt.log


# 拷贝模型modeling文件
MODELSCOPE_CACHE=/gemini-3/space/thu/hehaowei/modelscope_cache NCCL_IB_HCA=^mlx5_2 deepspeed --hostfile hostfile_hwmoe src/train.py --deepspeed examples/deepspeed/ds_z2_config_hhw.json --stage pt --model_name_or_path /gemini/space/thu/TeleChat-2B-A05B-exp16-ultrafineweb/  --train_from_scratch  --trust_remote_code --dataset ultrafineweb_en,ultrafineweb_zh --finetuning_type full --output_dir  /gemini-3/space/thu/hehaowei/LLaMA-Factory/saves/TeleChat-2B-A05B-exp16-ultrafineweb  --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 1 --save_steps 1000 --learning_rate 3e-4  --warmup_ratio 0.01  --num_train_epochs 1 --plot_loss --preprocessing_num_workers 40 --ddp_timeout 18000 --bf16 --cutoff_len 8192 --streaming --max_steps 30000 --flash_attn auto --do_train --preprocessing_num_workers 32 --cache_dir /gemini-3/space/thu/hehaowei/modelscope_cache/  2>&1 | tee TeleChat-2B-A05B-exp16-ultrafineweb.log

# 如果上面效果差，换qwen3 0.6B进行测试
